# Deep-Snap-Talk

This project involves generating image captions using deep learning techniques. The model is trained on the Flickr8k dataset, which contains 8,000 images and their corresponding captions. The project uses a pre-trained Xception model to extract features from the images, which are then fed into a deep learning model that generates the corresponding caption. The model architecture used in this project is an encoder-decoder model with an attention mechanism. The encoder is the Xception model, while the decoder is a LSTM-based language model. The decoder generates the captions word-by-word, and the attention mechanism helps the model to focus on the most relevant part of the image when generating each word. The project uses the Keras deep learning library for implementation. The code extracts the features from an input image using the Xception model and then generates a caption using the trained model. The output caption is displayed along with the input image using the matplotlib library. Overall, the project is a successful implementation of deep learning techniques for image captioning.
